{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e1a427-4dd5-45de-871f-6d5b35cba63c",
   "metadata": {},
   "source": [
    "# Question-Answering System on Private Documents Using OpenAI, Pinecone, and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9efe-1604-43c6-96f3-b4d5cbcb4ea7",
   "metadata": {},
   "source": [
    "GPT models are great at answering questions, but only on topics they have been trained on. What if you want GPT to answer questions about topics it hasn't been trained on? For example, about recent events after September 2021 for GPT-3.5 or GPT-4(not included in the training data) or about your non-public documents.\n",
    "\n",
    "**LLMs can learn new knowledge in two ways:**\n",
    "\n",
    "**1) Fine-Tuning on a training set:-** It is the most natural way to teach the model knowledge, but it can be time-consuming and expensive. It also builds long-term memory, which is not always necessary.\n",
    "   \n",
    "**2) Model Inputs:-** Model inputs means inserting the knowledge into an input message. For example, we can send an entire book or PDF document to the model as an input message, and then we can start asking questions on topics found in the input message. This is a good way to build short-term memory for the model. When we have a large corpus of text, it can be difficult to use model inputs because each model is limited to a maximum number of tokens, which in most cases is around 4000. We can not simply send the text from a 500-page document to the model because this will exceed the maximum number of tokens that the model supports.\n",
    "\n",
    "**The recommended approach is to use model inputs with embedded-based search.** Embeddings are simple to implement and work especially well with questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe8985-bced-4066-bfd8-4e6c09f9f1ef",
   "metadata": {},
   "source": [
    "## Question-Answering Pipeline\n",
    "\n",
    "**1) Prepare the document (Once per document)**\n",
    "\n",
    "   a)Load the data into LangChain Documents.\n",
    "   \n",
    "   b)Split the documents into chunks(short and self-contained sections).\n",
    "   \n",
    "   c)Embed the chunks into numeric vectors.(using an embedding model such as OpenAI's text-embedding-ada-002)\n",
    "   \n",
    "   d)Save the chunks and the embeddings to a vector database(such as Pinecone, Chroma, Milvus or Quadrant).\n",
    "\n",
    "**2) Search (Once per Query)**\n",
    "\n",
    "   a)Embed the user's question.(Given a user query, generate an embedding for the question using the same embedding model that was used for chunk embeddings)\n",
    "   \n",
    "   b)Using the question's embedding and the chunk embeddings, rank the vectors by similarity to the question's embedding(using cosine similarity or Euclidean distance). The nearest vectors represent chunks similar to the question.\n",
    "\n",
    "**3)Ask(once per query)**\n",
    "\n",
    "   a)Insert the question and the most relevant chunks (   obtained in step 2)b)  ) into a message to a GPT model.\n",
    "   \n",
    "   b)Return GPT's answer. (The GPT model will return an answer)\n",
    "\n",
    "   \n",
    "In this project we are building a complete quetion-answering application on custom data that follows the above pipeline. This Technique is also called Retrieval Augmentation because we retrieve relevant information from an external knowledge base and give that information to our LLM. The external knowledge base is our window into the world beyond the LLM's training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e14a67-7a5f-4008-b69a-758bb2b3263a",
   "metadata": {},
   "source": [
    "### 1) Prepare the document (Once per document)\n",
    "#### Loading Your Custom(Private) PDF Documents into LangChain\n",
    "The private data can be provided in different formats such as Pandas, Dataframes, PDFs, CSV or JSON files, HTML or office documents\n",
    "**LangChain provides with Document Loaders which load this data into documents.**  document loaders are used to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f06432-d8cd-483a-891f-90fd1c836e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36565f2c-2468-420b-a57e-16f088129e66",
   "metadata": {},
   "source": [
    "To load PDF files install the library named pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b2121-c29a-4ba6-a95d-3b23686fd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ab6da-55d8-47e7-832f-a5831fe7ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will take as an argument a PDF file and return its text . This function loads the PDFs using a library called pypdf into an array of documents, where each document contains the page_content and  meta_data with a page number.\n",
    "\n",
    "def load_document(file):\n",
    "    from langchain.document_loaders import PyPDFLoader       # By the way, the standard  recommendation is to put import statements at the top of the file, However there are cases when putting import statements inside the function is even better. When you move a function from one module to another, you will know that the function will continue to work, because it contains everything inside it.\n",
    "    print(f'Loading {file}')\n",
    "    loader = PyPDFLoader(file)    # note that it is also able to load online PDFs. just pass a URL to the PDF to PyPDFLoader()\n",
    "    data = loader.load()            # This will return a list of langchain documents, one document for each page.\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196278e-a0cd-4d29-8423-e1fbd7417190",
   "metadata": {},
   "source": [
    "##### Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9d912-a0da-487a-a001-401ffe37df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_document()                # note that it is also able to load online PDFs. just pass a URL to the PDF to PyPDFLoader().\n",
    "print(data[1].page_content)         # The data is splitted by pages and you can use indexes to display a specific page. This is second page because it starts from zero.\n",
    "print(data[1].metadata)             # metadata is a dictionary.\n",
    "print(f'You have {len(data)} pages in your data')         # Number of pages\n",
    "print(f' There are {len(data[1].page_content)} characters in the page')                      #Number of characters in one page\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
