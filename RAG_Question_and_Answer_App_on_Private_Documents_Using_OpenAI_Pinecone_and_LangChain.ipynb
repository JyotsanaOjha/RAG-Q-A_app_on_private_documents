{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e1a427-4dd5-45de-871f-6d5b35cba63c",
   "metadata": {},
   "source": [
    "# Project: Question-Answering System on Private Documents Using OpenAI, Pinecone, and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9efe-1604-43c6-96f3-b4d5cbcb4ea7",
   "metadata": {},
   "source": [
    "GPT models are great at answering questions, but only on topics they have been trained on. What if you want GPT to answer questions about topics it hasn't been trained on? For example, about recent events after September 2021 for GPT-3.5 or GPT-4(not included in the training data) or about your non-public documents.\n",
    "\n",
    "**LLMs can learn new knowledge in two ways:**\n",
    "\n",
    "**1) Fine-Tuning on a training set:-** It is the most natural way to teach the model knowledge, but it can be time-consuming and expensive. It also builds long-term memory, which is not always necessary.\n",
    "   \n",
    "**2) Model Inputs:-** Model inputs means inserting the knowledge into an input message. For example, we can send an entire book or PDF document to the model as an input message, and then we can start asking questions on topics found in the input message. This is a good way to build short-term memory for the model. When we have a large corpus of text, it can be difficult to use model inputs because each model is limited to a maximum number of tokens, which in most cases is around 4000. We can not simply send the text from a 500-page document to the model because this will exceed the maximum number of tokens that the model supports.\n",
    "\n",
    "**The recommended approach is to use model inputs with embedded-based search.** Embeddings are simple to implement and work especially well with questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe8985-bced-4066-bfd8-4e6c09f9f1ef",
   "metadata": {},
   "source": [
    "## Question-Answering Pipeline\n",
    "\n",
    "**1) Prepare the document (Once per document)**\n",
    "\n",
    "   a)Load the data into LangChain Documents.\n",
    "   \n",
    "   b)Split the documents into chunks(short and self-contained sections).\n",
    "   \n",
    "   c)Embed the chunks into numeric vectors.(using an embedding model such as OpenAI's text-embedding-ada-002)\n",
    "   \n",
    "   d)Save the chunks and the embeddings to a vector database(such as Pinecone, Chroma, Milvus or Quadrant).\n",
    "\n",
    "**2) Search (Once per Query)**\n",
    "\n",
    "   a)Embed the user's question.(Given a user query, generate an embedding for the question using the same embedding model that was used for chunk embeddings)\n",
    "   \n",
    "   b)Using the question's embedding and the chunk embeddings, rank the vectors by similarity to the question's embedding(using cosine similarity or Euclidean distance). The nearest vectors represent chunks similar to the question.\n",
    "\n",
    "**3)Ask(once per query)**\n",
    "\n",
    "   a)Insert the question and the most relevant chunks (   obtained in step 2)b)  ) into a message to a GPT model.\n",
    "   \n",
    "   b)Return GPT's answer. (The GPT model will return an answer)\n",
    "\n",
    "   \n",
    "In this project we are building a complete quetion-answering application on custom data that follows the above pipeline. This Technique is also called Retrieval Augmentation because we retrieve relevant information from an external knowledge base and give that information to our LLM. The external knowledge base is our window into the world beyond the LLM's training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e14a67-7a5f-4008-b69a-758bb2b3263a",
   "metadata": {},
   "source": [
    "### 1) Prepare the document (Once per document)\n",
    "#### Loading Your Custom(Private) PDF Documents into LangChain documents\n",
    "The private data can be provided in different formats such as Pandas, Dataframes, PDFs, CSV or JSON files, HTML or office documents\n",
    "**LangChain provides with Document Loaders which load this data into documents.**  document loaders are used to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f06432-d8cd-483a-891f-90fd1c836e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a59d8-1db9-4c18-93c7-6e1b203f5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82cc0e9-c8d5-456e-b4d5-91758d22477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0375fd-2155-49fc-a9d5-347952f5ce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36565f2c-2468-420b-a57e-16f088129e66",
   "metadata": {},
   "source": [
    "To load PDF files install the library named pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b2121-c29a-4ba6-a95d-3b23686fd9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c913c0a8-d5ea-4c7d-83ba-5b9ee278e056",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e106aee-635e-4dae-99f2-a14d13f378d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ab6da-55d8-47e7-832f-a5831fe7ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will take as an argument a PDF file and return its text . This function loads the PDFs using a library called pypdf into an array of documents, where each document contains the page_content and  meta_data with a page number.\n",
    "\n",
    "# def load_document(file):\n",
    "#     from langchain.document_loaders import PyPDFLoader       # By the way, the standard  recommendation is to put import statements at the top of the file, However there are cases when putting import statements inside the function is even better. When you move a function from one module to another, you will know that the function will continue to work, because it contains everything inside it.\n",
    "#     print(f'Loading {file}')\n",
    "#     loader = PyPDFLoader(file)    # note that it is also able to load online PDFs. just pass a URL to the PDF to PyPDFLoader()\n",
    "#     data = loader.load()            # This will return a list of langchain documents, one document for each page.\n",
    "#     return data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e0703-9a8d-44f3-8d29-5be60ba058f7",
   "metadata": {},
   "source": [
    "In the above code, we can load PDF files into langchain documents. However our private unstructured data isn't limited to PDF format, it can be found in various other formats such as office documents, Google Docs, and many more. In the following code, we are loading only pdf and docx formats document formats into the langchain document. for this, we will check the file's extension and load it using the specific langchain loader based on its extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9922337d-f0fe-4905-a56b-8b521bc93cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform loaders (pdf, docx)\n",
    "    #(which transforms or load data from a specific format into the langchain document format)\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)   # splitting the file name into name and extension. We can print name and extension if we want to see their values.\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)  \n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "        \n",
    "    data = loader.load()            \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "#Public Service loader (Wikipedia)\n",
    "    #(Loading data from online public services into langchain. Here we don't deal with files but with different protocols or APIs that connect to those services. Since the format and code differ for each service, I would create a unique function for each dataset or service loader that I want to support in my application.)\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)    #load_max_docs can be used to limit the number of downloaded documents. for this we can use the hard-coded value or add a third argument to the function.\n",
    "    data = loader.load() \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061eaa7-33be-40ae-9d57-e5146d33b246",
   "metadata": {},
   "source": [
    "#### Split the documents into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a53d1-10e4-4cf8-b2ae-73a24b7c4ec4",
   "metadata": {},
   "source": [
    "After loading our custom or private data into langchain documents, The next step is to split or chunk the documents into smaller parts in the context of building the LLM applications.\n",
    "\n",
    "Chunking is the breaking down of large pieces of text into smaller segments. It is an essential technique that helps optimize the relevance of the content we get back from a vector database.  \n",
    "\n",
    "By applying an effective chunking strategy, we can make sure that our search results accurately capture the essence of the user's query. If our chunks are too small or too large, It may lead to imprecise search results or missed opportunities to surface relevant content.\n",
    "\n",
    "As a rule of thumb, if a chunk of text makes sense without the surrounding context to a human, it will also make sense to the language model. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensure that the search results are accurate and relevant.\n",
    "\n",
    "When a full paragraph or document is embedded, The embedding process considers both the overall context and the relationships between the sentences and the phrases within the text. This can result in more comprehensive vector representation that captures the broader meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9eb5371e-67c3-441a-a369-72b737e8098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter     # langchain provides many text splitters, but RecursiveCharacterTextSplitter is recommended for generic text. By default, the characters it tries to split on are \\\\n  \\n and whitespace.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)   # it returns a list of dacuments.\n",
    "   # chunks = text_splitter.create_documents(data)    # use this \"text_splitter.create_documents()\" method, instead of \"text_splitter.split_documents()\", when it is not already splitted in pages. It depends on how you have loaded the data.\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a0530-aa7b-4610-91f3-f3c294808b9e",
   "metadata": {},
   "source": [
    "#### calculating the embedding cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa4b66-0e51-4db4-bb17-81d4af3c36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding cost in USD: {total_tokens / 1000 * 0.0004:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c2ab8-91f0-449d-89be-7978424a8006",
   "metadata": {},
   "source": [
    "#### Embedding the chunks into numeric vectors and uploading the chunks and the embeddings to a vector database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a92348-07fa-433c-9c1e-772739ebbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will create an index, if the index doesn't exist, embed the chunks and add both the chunks and embeddings into the pinecone index for fast retrieval and similarity search.\n",
    "# If the index already exists, the function will load the embeddings from that index.\n",
    "\n",
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import ServerlessSpec\n",
    "   \n",
    "\n",
    "    pc = pinecone.Pinecone()   # if the API key is not provided in .env file then, we can write as follows:  pc = pinecone.Pinecone(api_key='YOUR_API_KEY') \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ...', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536, \n",
    "            metric='cosine', \n",
    "            spec=pinecone.ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=\"us-east-1\"\n",
    "            ) \n",
    "        )\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)  # This method is processing the input documents(the chunks), generating the embeddings using the provided OpenAI's embeddings instance, inserting the embeddings into the index and returning a new pinecone vector store object.\n",
    "        print('Ok')\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "##If you want to create a new Pod Based index instead of a serverless one, then use the following configuration:\n",
    "#from pinecone import PodSpec\n",
    "# pc.create_index(\n",
    "#             name=index_name,\n",
    "#             dimension=1536,\n",
    "#             metric='cosine'\n",
    "#             spec=PodSpec(environment='gcp-starter')  #gcp stands for google cloud platform\n",
    "\n",
    "#         )\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1befca39-30a4-44af-a3a1-8dc78ee0f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are creating a function that deletes a pinecone index or all the indexes. Because the pinecone free tier supports only one index, it could be necessary to delete the existing index frequently.\n",
    "\n",
    "#When we are using pinecone free tire and we want to avoid getting an error when we try to create a new index then we have to make sure that there are no pinecone indexes. So we will remove all indexes first.\n",
    "\n",
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ....')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ....', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97126a-f873-4a4c-aff6-163e47820f9b",
   "metadata": {},
   "source": [
    "##### Asking and Getting Answers\n",
    "The chunks represent the answer, but you can't give them to users like this. we need the answers in natural language. That's where the LLM comes in. In the following function, first, we'll retrieve the most relevant chunks of text from our vector database, and then we'll feed those chunks to the LLMs to get the final answer.\n",
    "\n",
    "Note: Also check the previous project (Question-Answer about content of private document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6035cc-79af-4b7f-b270-df1c5e55b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(model='gpt-4o-mini', temperature=1)\n",
    "    \n",
    "    # here I am exposing the index in a retriever interface. The retriever interface is a generic interface that makes it easy to combine documents with language models.\n",
    "    #search_kwargs takes the value as a dictionary. The key k has a value as an integer\n",
    "    # {'k': 3} means that it will return the three most similar chunks to the user's query \n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    \n",
    "    # Finally, creating a chain to answer the questions. The default chain_type=\"stuff\" uses all of the text from the documents in the prompt.\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    #Now run the chain\n",
    "    answer = chain.invoke(q)\n",
    "    return answer\n",
    "\n",
    "\n",
    "#index = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)\n",
    "# docs = index.similarity_search('Enter query here', include_metadata = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196278e-a0cd-4d29-8423-e1fbd7417190",
   "metadata": {},
   "source": [
    "##### Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9d912-a0da-487a-a001-401ffe37df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_document('files/Learn_Java.pdf')                # note that it is also able to load online PDFs. just pass a URL to the PDF to PyPDFLoader().\n",
    "                                                                        \n",
    "print(data[20].page_content)         # The data is splitted by pages and you can use indexes to display a specific page. This is second page because it starts from zero.\n",
    "print(data[20].metadata)             # metadata is a dictionary.\n",
    "print(f'You have {len(data)} pages in your data')         # Number of pages\n",
    "print(f' There are {len(data[20].page_content)} characters in the page')                      #Number of characters in one page\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5f431-9345-4624-8d78-2251bd081b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_document('files/java_notes.docx')     # here data is a list with a single element and content is the page_content attribute\n",
    "\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77954882-a3c8-44b7-82c5-3bc85dfaeac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The modern Olympic Games (OG; or Olympics; French: Jeux olympiques, JO) are the leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of competitions. The Olympic Games are considered the world's foremost sports competition with more than 200 teams, representing sovereign states and territories, participating. By default, the Games generally substitute for any world championships during the year in which they take place (however, each class usually maintains its own records). The Olympic Games are held every four years. Since 1994, they have alternated between the Summer and Winter Olympics every two years during the four-year Olympiad.\n",
      "Their creation was inspired by the ancient Olympic Games, held in Olympia, Greece from the 8th century BC to the 4th century AD. Baron Pierre de Coubertin founded the International Olympic Committee (IOC) in 1894, leading to the first modern Games in Athens in 1896. The IOC is the governing body of the Olympic Movement, which encompasses all entities and individuals involved in the Olympic Games. The Olympic Charter defines their structure and authority.\n",
      "The evolution of the Olympic Movement during the 20th and 21st centuries has resulted in numerous changes to the Olympic Games. Some of these adjustments include the creation of the Winter Olympic Games for snow and ice sports, the Paralympic Games for athletes with disabilities, the Youth Olympic Games for athletes aged 14 to 18, the five Continental Games (Pan American, African, Asian, European, and Pacific), and the World Games for sports that are not contested in the Olympic Games. The IOC also endorses the Deaflympics and the Special Olympics. The IOC has needed to adapt to a variety of economic, political, and technological advancements. The abuse of amateur rules by the Eastern Bloc nations prompted the IOC to shift away from pure amateurism, as envisioned by Coubertin, to the acceptance of professional athletes participating at the Games. The growing importance of mass media has created the issue of corporate sponsorship and general commercialisation of the Games. World Wars I and II led to the cancellation of the 1916, 1940, and 1944 Olympics; large-scale boycotts during the Cold War limited participation in the 1980 and 1984 Olympics; and the 2020 Olympics were postponed until 2021 as a result of the COVID-19 restrictions.\n",
      "The Olympic Movement consists of international sports federations (IFs), National Olympic Committees (NOCs), and organising committees for each specific Olympic Games. As the decision-making body, the IOC is responsible for choosing the host city for each Games, and organises and funds the Games according to the Olympic Charter. The IOC also determines the Olympic programme, consisting of the sports to be contested at the Games. There are several Olympic rituals and symbols, such as the Olympic flag, torch, and opening and closing ceremonies. Over 14,000 athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined, in 40 different sports and 448 events. The first-, second-, and third-place finishers in each event receive Olympic medals: gold, silver, and bronze, respectively.\n",
      "The Games have grown to the point that nearly every nation is now represented; colonies and overseas territories are often allowed to field their own teams. This growth has created numerous challenges and controversies, including boycotts, doping, bribery, and terrorism. Every two years, the Olympics and its media exposure provide athletes with the chance to attain national and international fame. The Games also provide an opportunity for the host city and country to showcase themselves to the world.\n",
      "The Olympic Games have become a significant global event, fostering international cooperation and cultural exchange. At the same time, hosting the Olympic Games can also bring significant economic benefits and challenges \n"
     ]
    }
   ],
   "source": [
    "#data = load_from_wikipedia('Chandrayaan-3')\n",
    "data = load_from_wikipedia('Olympics', 'en')  #Important Note: (Now I am using 'gpt-4o-mini' with a knowledge cut-off date of October 2023.)The training data for GPT-4 was cut off in September 2021. Chandrayaan-3 was launched in July 2023. So it was not included in the GPT-4 training data. Without loading the data from external sources, LLMs like gpt-3.5-turbo or gpt-4 have no knowledge of it.\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "825620a6-9437-4439-9a93-a00fa96bbcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "Sydney was selected as the host city for the 2000 Games in 1993. Teams from 199 countries participated in the 2000 Games, which were the first to feature at least 300 events in its official sports program. The Games' cost was estimated to be A$6.6\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))\n",
    "print(chunks[20].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd603071-0bbe-4c04-a711-dacea35baa69",
   "metadata": {},
   "source": [
    "We are using Openai's Model \"text-embedding-ada-002\" which has a cost. So in the following cell, we are calculating the embedding costs using tiktoken library, in advance to avoid any surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce21e1a-d89c-4d6d-8023-509eb5eacbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abddb23-7906-45ef-8794-654c92898435",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51789ae2-53c0-4b03-aad7-31b493a19b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index olympics already exists. Loading embeddings ...Ok\n",
      "<langchain_community.vectorstores.pinecone.Pinecone object at 0x0000017DE650BD90>\n"
     ]
    }
   ],
   "source": [
    "index_name = 'olympics'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd4e52-f415-4e86-ae49-b040be3eb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = \"when the propulsion module returned to a high Earth orbit from lunar orbit?\"\n",
    "q = \"from where chandrayaan 3 was launched?\"\n",
    "print(vector_store)\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e287f8-2c98-4f3f-8b04-1e7396589a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a loop so that the user can send questions to the application continuously\n",
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting.....bye bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "\n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f\"\\nAnswer: {answer['result']}\")\n",
    "    print(f'\\n{\"-\" * 50}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f38ced3-1636-4108-b413-986d9b76f853",
   "metadata": {},
   "source": [
    "##### Adding Memory (Chat History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98d506ee-bf91-48fe-8a3b-7c662e263c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Earlier in ask_and_get_answer() method we used the RetrievalQA chain to ask questions against a vector store. That works well but has one disadvantage. It fails to preserve conversational history for any follow-up question. Follow-up questions can contain references to past chat history. So in the following code, we will save the chat history to ask follow-up questions.\n",
    "#Instead of using the  RetrievalQA chain, here we are using another chain called ConversationalRetrievalChain.This chain is used to have a conversation based on the retrieved documents. \n",
    "#ConversationBufferMemory class acts as a buffer for storing conversation memory.\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "#In the RAG system, external data is retrieved and passed to the LLM during the generation step. A retriever is a crucial component that helps LLM find and access relevant information. It does this by searching for relevant data and retrieving the information. In this example, the retriever will search by similarity and will retrieve the top k most similar chunks of data.\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)  # The memory will be automatically updated with the questions and the answers. Memory is specifically designed to store and manage conversation history within the langchain application. memory_key='chat_history' gives your memory a label. When retrieving or interacting with the stored conversation, we'll use the key \"chat_history\".\n",
    "\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",   #chain_type=\"stuff\" means use all of the text from the documents.\n",
    "        memory=memory,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98318b71-131e-4dd6-a079-40e619a2dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(q, chain):\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6689039-fa22-4904-afba-d63fbbc2de48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "torch, and opening and closing ceremonies. Over 14,000 athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined, in 40 different sports and 448 events. The first-, second-, and third-place finishers in each event receive Olympic\n",
      "\n",
      "competitions. The Olympic Games are considered the world's foremost sports competition with more than 200 teams, representing sovereign states and territories, participating. By default, the Games generally substitute for any world championships during\n",
      "\n",
      "The modern Olympic Games (OG; or Olympics; French: Jeux olympiques, JO) are the leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of\n",
      "\n",
      "(Chinese: 北京2008; pinyin: Běijīng èr líng líng bā), were an international multisport event held from 8 to 24 August 2008, in Beijing, China. A total of 10,942 athletes from 204 National Olympic Committees (NOCs) competed in 28 sports and 302 events, one\n",
      "\n",
      "The evolution of the Olympic Movement during the 20th and 21st centuries has resulted in numerous changes to the Olympic Games. Some of these adjustments include the creation of the Winter Olympic Games for snow and ice sports, the Paralympic Games for\n",
      "Human: In how many sports, athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'In how many sports, athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined?', 'chat_history': [HumanMessage(content='In how many sports, athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined?'), AIMessage(content='Athletes competed in 40 different sports at the 2020 Summer Olympics and 2022 Winter Olympics combined.')], 'answer': 'Athletes competed in 40 different sports at the 2020 Summer Olympics and 2022 Winter Olympics combined.'}\n",
      "--------------------------------------------------\n",
      "Athletes competed in 40 different sports at the 2020 Summer Olympics and 2022 Winter Olympics combined.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "q = \"In how many sports, athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined?\"\n",
    "result = ask_question(q, crc)\n",
    "print(result)\n",
    "print('-' * 50)\n",
    "print(result['answer'])\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882056d4-10ef-40e6-b5be-9646392637d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: In how many sports, athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined?\n",
      "Assistant: Athletes competed in 40 different sports at the 2020 Summer Olympics and 2022 Winter Olympics combined.\n",
      "Follow Up Input: Multiply that number by 3.\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "torch, and opening and closing ceremonies. Over 14,000 athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined, in 40 different sports and 448 events. The first-, second-, and third-place finishers in each event receive Olympic\n",
      "\n",
      "The evolution of the Olympic Movement during the 20th and 21st centuries has resulted in numerous changes to the Olympic Games. Some of these adjustments include the creation of the Winter Olympic Games for snow and ice sports, the Paralympic Games for\n",
      "\n",
      "The modern Olympic Games (OG; or Olympics; French: Jeux olympiques, JO) are the leading international sporting events featuring summer and winter sports competitions in which thousands of athletes from around the world participate in a variety of\n",
      "\n",
      "(Chinese: 北京2008; pinyin: Běijīng èr líng líng bā), were an international multisport event held from 8 to 24 August 2008, in Beijing, China. A total of 10,942 athletes from 204 National Olympic Committees (NOCs) competed in 28 sports and 302 events, one\n",
      "\n",
      "competitions. The Olympic Games are considered the world's foremost sports competition with more than 200 teams, representing sovereign states and territories, participating. By default, the Games generally substitute for any world championships during\n",
      "Human: What is the result of multiplying the number of sports athletes competed in at the 2020 Summer Olympics and 2022 Winter Olympics combined by 3?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'Multiply that number by 3.', 'chat_history': [HumanMessage(content='In how many sports, athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined?'), AIMessage(content='Athletes competed in 40 different sports at the 2020 Summer Olympics and 2022 Winter Olympics combined.'), HumanMessage(content='Multiply that number by 3.'), AIMessage(content='The number of sports at the 2020 Summer Olympics and 2022 Winter Olympics combined is 40. Multiplying this by 3 gives you 120.')], 'answer': 'The number of sports at the 2020 Summer Olympics and 2022 Winter Olympics combined is 40. Multiplying this by 3 gives you 120.'}\n",
      "--------------------------------------------------\n",
      "The number of sports at the 2020 Summer Olympics and 2022 Winter Olympics combined is 40. Multiplying this by 3 gives you 120.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "q = \"Multiply that number by 3.\"\n",
    "result = ask_question(q, crc)\n",
    "print(result)\n",
    "print('-' * 50)\n",
    "print(result['answer'])\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cba55cf-342a-40ff-aec2-34fe543a3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: In how many sports, athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined?\n",
      "Assistant: Athletes competed in 40 different sports at the 2020 Summer Olympics and 2022 Winter Olympics combined.\n",
      "Human: Multiply that number by 3.\n",
      "Assistant: The number of sports at the 2020 Summer Olympics and 2022 Winter Olympics combined is 40. Multiplying this by 3 gives you 120.\n",
      "Follow Up Input: divide that number by 100.\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "world.\n",
      "\n",
      "== Organization ==\n",
      "\n",
      "\n",
      "=== Bid ===\n",
      "\n",
      "The 2008 Summer Olympics (Chinese: 2008年夏季奥运会; pinyin: Èr Líng Líng Bā Nián Xiàjì Àoyùnhuì), officially the Games of the XXIX Olympiad (Chinese: 第二十九届夏季奥林匹克运动会; pinyin: Dì Èrshíjiǔ Jiè Xiàjì Àolínpǐkè Yùndònghuì) and officially branded as Beijing 2008\n",
      "\n",
      "An unprecedented 87 countries won at least one medal during the 2008 Games. Host nation China won the most gold medals (48), and became the seventh different team to top an overall Summer Olympics medal tally, winning a total of 100 medals overall. The\n",
      "\n",
      "the year in which they take place (however, each class usually maintains its own records). The Olympic Games are held every four years. Since 1994, they have alternated between the Summer and Winter Olympics every two years during the four-year Olympiad.\n",
      "Human: What is 120 divided by 100?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'question': 'divide that number by 100.', 'chat_history': [HumanMessage(content='In how many sports, athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined?'), AIMessage(content='Athletes competed in 40 different sports at the 2020 Summer Olympics and 2022 Winter Olympics combined.'), HumanMessage(content='Multiply that number by 3.'), AIMessage(content='The number of sports at the 2020 Summer Olympics and 2022 Winter Olympics combined is 40. Multiplying this by 3 gives you 120.'), HumanMessage(content='divide that number by 100.'), AIMessage(content='120 divided by 100 is 1.2.')], 'answer': '120 divided by 100 is 1.2.'}\n",
      "--------------------------------------------------\n",
      "120 divided by 100 is 1.2.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "q = \"divide that number by 100.\"\n",
    "result = ask_question(q, crc)\n",
    "print(result)\n",
    "print('-' * 50)\n",
    "print(result['answer'])\n",
    "print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71217032-deea-41b8-b60c-438d22ca3997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='In how many sports, athletes competed at the 2020 Summer Olympics and 2022 Winter Olympics combined?'\n",
      "content='Athletes competed in 40 different sports at the 2020 Summer Olympics and 2022 Winter Olympics combined.'\n",
      "content='Multiply that number by 3.'\n",
      "content='The number of sports at the 2020 Summer Olympics and 2022 Winter Olympics combined is 40. Multiplying this by 3 gives you 120.'\n",
      "content='divide that number by 100.'\n",
      "content='120 divided by 100 is 1.2.'\n"
     ]
    }
   ],
   "source": [
    "# To display the chat history, which contains all the questions and their answers, Iterate over the content of the chat history key as follows:-\n",
    "for item in result['chat_history']:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fffce99-275d-4457-88d0-11bc97d6072f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
