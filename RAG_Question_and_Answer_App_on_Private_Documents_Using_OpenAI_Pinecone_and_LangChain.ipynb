{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58e1a427-4dd5-45de-871f-6d5b35cba63c",
   "metadata": {},
   "source": [
    "# Project: Question-Answering System on Private Documents Using OpenAI, Pinecone, and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346d9efe-1604-43c6-96f3-b4d5cbcb4ea7",
   "metadata": {},
   "source": [
    "GPT models are great at answering questions, but only on topics they have been trained on. What if you want GPT to answer questions about topics it hasn't been trained on? For example, about recent events after September 2021 for GPT-3.5 or GPT-4(not included in the training data) or about your non-public documents.\n",
    "\n",
    "**LLMs can learn new knowledge in two ways:**\n",
    "\n",
    "**1) Fine-Tuning on a training set:-** It is the most natural way to teach the model knowledge, but it can be time-consuming and expensive. It also builds long-term memory, which is not always necessary.\n",
    "   \n",
    "**2) Model Inputs:-** Model inputs means inserting the knowledge into an input message. For example, we can send an entire book or PDF document to the model as an input message, and then we can start asking questions on topics found in the input message. This is a good way to build short-term memory for the model. When we have a large corpus of text, it can be difficult to use model inputs because each model is limited to a maximum number of tokens, which in most cases is around 4000. We can not simply send the text from a 500-page document to the model because this will exceed the maximum number of tokens that the model supports.\n",
    "\n",
    "**The recommended approach is to use model inputs with embedded-based search.** Embeddings are simple to implement and work especially well with questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe8985-bced-4066-bfd8-4e6c09f9f1ef",
   "metadata": {},
   "source": [
    "## Question-Answering Pipeline\n",
    "\n",
    "**1) Prepare the document (Once per document)**\n",
    "\n",
    "   a)Load the data into LangChain Documents.\n",
    "   \n",
    "   b)Split the documents into chunks(short and self-contained sections).\n",
    "   \n",
    "   c)Embed the chunks into numeric vectors.(using an embedding model such as OpenAI's text-embedding-ada-002)\n",
    "   \n",
    "   d)Save the chunks and the embeddings to a vector database(such as Pinecone, Chroma, Milvus or Quadrant).\n",
    "\n",
    "**2) Search (Once per Query)**\n",
    "\n",
    "   a)Embed the user's question.(Given a user query, generate an embedding for the question using the same embedding model that was used for chunk embeddings)\n",
    "   \n",
    "   b)Using the question's embedding and the chunk embeddings, rank the vectors by similarity to the question's embedding(using cosine similarity or Euclidean distance). The nearest vectors represent chunks similar to the question.\n",
    "\n",
    "**3)Ask(once per query)**\n",
    "\n",
    "   a)Insert the question and the most relevant chunks (   obtained in step 2)b)  ) into a message to a GPT model.\n",
    "   \n",
    "   b)Return GPT's answer. (The GPT model will return an answer)\n",
    "\n",
    "   \n",
    "In this project we are building a complete quetion-answering application on custom data that follows the above pipeline. This Technique is also called Retrieval Augmentation because we retrieve relevant information from an external knowledge base and give that information to our LLM. The external knowledge base is our window into the world beyond the LLM's training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e14a67-7a5f-4008-b69a-758bb2b3263a",
   "metadata": {},
   "source": [
    "### 1) Prepare the document (Once per document)\n",
    "#### Loading Your Custom(Private) PDF Documents into LangChain documents\n",
    "The private data can be provided in different formats such as Pandas, Dataframes, PDFs, CSV or JSON files, HTML or office documents\n",
    "**LangChain provides with Document Loaders which load this data into documents.**  document loaders are used to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f06432-d8cd-483a-891f-90fd1c836e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8a59d8-1db9-4c18-93c7-6e1b203f5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a82cc0e9-c8d5-456e-b4d5-91758d22477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0375fd-2155-49fc-a9d5-347952f5ce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36565f2c-2468-420b-a57e-16f088129e66",
   "metadata": {},
   "source": [
    "To load PDF files install the library named pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b49b2121-c29a-4ba6-a95d-3b23686fd9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c913c0a8-d5ea-4c7d-83ba-5b9ee278e056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e106aee-635e-4dae-99f2-a14d13f378d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ab6da-55d8-47e7-832f-a5831fe7ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will take as an argument a PDF file and return its text . This function loads the PDFs using a library called pypdf into an array of documents, where each document contains the page_content and  meta_data with a page number.\n",
    "\n",
    "# def load_document(file):\n",
    "#     from langchain.document_loaders import PyPDFLoader       # By the way, the standard  recommendation is to put import statements at the top of the file, However there are cases when putting import statements inside the function is even better. When you move a function from one module to another, you will know that the function will continue to work, because it contains everything inside it.\n",
    "#     print(f'Loading {file}')\n",
    "#     loader = PyPDFLoader(file)    # note that it is also able to load online PDFs. just pass a URL to the PDF to PyPDFLoader()\n",
    "#     data = loader.load()            # This will return a list of langchain documents, one document for each page.\n",
    "#     return data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e0703-9a8d-44f3-8d29-5be60ba058f7",
   "metadata": {},
   "source": [
    "In the above code, we can load PDF files into langchain documents. However our private unstructured data isn't limited to PDF format, it can be found in various other formats such as office documents, Google Docs, and many more. In the following code, we are loading only pdf and docx formats document formats into the langchain document. for this, we will check the file's extension and load it using the specific langchain loader based on its extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9922337d-f0fe-4905-a56b-8b521bc93cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform loaders (pdf, docx)\n",
    "    #(which transforms or load data from a specific format into the langchain document format)\n",
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)   # splitting the file name into name and extension. We can print name and extension if we want to see their values.\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = PyPDFLoader(file)  \n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading {file}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported!')\n",
    "        return None\n",
    "        \n",
    "    data = loader.load()            \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "#Public Service loader (Wikipedia)\n",
    "    #(Loading data from online public services into langchain. Here we don't deal with files but with different protocols or APIs that connect to those services. Since the format and code differ for each service, I would create a unique function for each dataset or service loader that I want to support in my application.)\n",
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)    #load_max_docs can be used to limit the number of downloaded documents. for this we can use the hard-coded value or add a third argument to the function.\n",
    "    data = loader.load() \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1061eaa7-33be-40ae-9d57-e5146d33b246",
   "metadata": {},
   "source": [
    "#### Split the documents into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a53d1-10e4-4cf8-b2ae-73a24b7c4ec4",
   "metadata": {},
   "source": [
    "After loading our custom or private data into langchain documents, The next step is to split or chunk the documents into smaller parts in the context of building the LLM applications.\n",
    "\n",
    "Chunking is the breaking down of large pieces of text into smaller segments. It is an essential technique that helps optimize the relevance of the content we get back from a vector database.  \n",
    "\n",
    "By applying an effective chunking strategy, we can make sure that our search results accurately capture the essence of the user's query. If our chunks are too small or too large, It may lead to imprecise search results or missed opportunities to surface relevant content.\n",
    "\n",
    "As a rule of thumb, if a chunk of text makes sense without the surrounding context to a human, it will also make sense to the language model. Therefore, finding the optimal chunk size for the documents in the corpus is crucial to ensure that the search results are accurate and relevant.\n",
    "\n",
    "When a full paragraph or document is embedded, The embedding process considers both the overall context and the relationships between the sentences and the phrases within the text. This can result in more comprehensive vector representation that captures the broader meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb5371e-67c3-441a-a369-72b737e8098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter     # langchain provides many text splitters, but RecursiveCharacterTextSplitter is recommended for generic text. By default, the characters it tries to split on are \\\\n  \\n and whitespace.\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = text_splitter.split_documents(data)   # it returns a list of dacuments.\n",
    "   # chunks = text_splitter.create_documents(data)    # use this \"text_splitter.create_documents()\" method, instead of \"text_splitter.split_documents()\", when it is not already splitted in pages. It depends on how you have loaded the data.\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36a0530-aa7b-4610-91f3-f3c294808b9e",
   "metadata": {},
   "source": [
    "#### calculating the embedding cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa4b66-0e51-4db4-bb17-81d4af3c36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total Tokens: {total_tokens}')\n",
    "    print(f'Embedding cost in USD: {total_tokens / 1000 * 0.0004:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c2ab8-91f0-449d-89be-7978424a8006",
   "metadata": {},
   "source": [
    "#### Embedding the chunks into numeric vectors and uploading the chunks and the embeddings to a vector database (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a92348-07fa-433c-9c1e-772739ebbac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will create an index, if the index doesn't exist, embed the chunks and add both the chunks and embeddings into the pinecone index for fast retrieval and similarity search.\n",
    "# If the index already exists, the function will load the embeddings from that index.\n",
    "\n",
    "def insert_or_fetch_embeddings(index_name, chunks):\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import ServerlessSpec\n",
    "   \n",
    "\n",
    "    pc = pinecone.Pinecone()   # if the API key is not provided in .env file then, we can write as follows:  pc = pinecone.Pinecone(api_key='YOUR_API_KEY') \n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f'Index {index_name} already exists. Loading embeddings ...', end='')\n",
    "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536, \n",
    "            metric='cosine', \n",
    "            spec=pinecone.ServerlessSpec(\n",
    "                    cloud=\"aws\",\n",
    "                    region=\"us-east-1\"\n",
    "            ) \n",
    "        )\n",
    "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)  # This method is processing the input documents(the chunks), generating the embeddings using the provided OpenAI's embeddings instance, inserting the embeddings into the index and returning a new pinecone vector store object.\n",
    "        print('Ok')\n",
    "\n",
    "    return vector_store\n",
    "\n",
    "##If you want to create a new Pod Based index instead of a serverless one, then use the following configuration:\n",
    "#from pinecone import PodSpec\n",
    "# pc.create_index(\n",
    "#             name=index_name,\n",
    "#             dimension=1536,\n",
    "#             metric='cosine'\n",
    "#             spec=PodSpec(environment='gcp-starter')  #gcp stands for google cloud platform\n",
    "\n",
    "#         )\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1befca39-30a4-44af-a3a1-8dc78ee0f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are creating a function that deletes a pinecone index or all the indexes. Because the pinecone free tier supports only one index, it could be necessary to delete the existing index frequently.\n",
    "\n",
    "#When we are using pinecone free tire and we want to avoid getting an error when we try to create a new index then we have to make sure that there are no pinecone indexes. So we will remove all indexes first.\n",
    "\n",
    "def delete_pinecone_index(index_name='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print('Deleting all indexes ....')\n",
    "        for index in indexes:\n",
    "            pc.delete_index(index)\n",
    "        print('Ok')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name} ....', end='')\n",
    "        pc.delete_index(index_name)\n",
    "        print('Ok')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97126a-f873-4a4c-aff6-163e47820f9b",
   "metadata": {},
   "source": [
    "##### Asking and Getting Answers\n",
    "The chunks represent the answer, but you can't give them to users like this. we need the answers in natural language. That's where the LLM comes in. In the following function, first, we'll retrieve the most relevant chunks of text from our vector database, and then we'll feed those chunks to the LLMs to get the final answer.\n",
    "\n",
    "Note: Also check the previous project (Question-Answer about content of private document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6035cc-79af-4b7f-b270-df1c5e55b909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    \n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "    \n",
    "    # here I am exposing the index in a retriever interface. The retriever interface is a generic interface that makes it easy to combine documents with language models.\n",
    "    #search_kwargs takes the value as a dictionary. The key k has a value as an integer\n",
    "    # {'k': 3} means that it will return the three most similar chunks to the user's query \n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 3})\n",
    "    \n",
    "    # Finally, creating a chain to answer the questions. The default chain_type=\"stuff\" uses all of the text from the documents in the prompt.\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    #Now run the chain\n",
    "    answer = chain.invoke(q)\n",
    "    return answer\n",
    "\n",
    "\n",
    "#index = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)\n",
    "# docs = index.similarity_search('Enter query here', include_metadata = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196278e-a0cd-4d29-8423-e1fbd7417190",
   "metadata": {},
   "source": [
    "##### Running Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9d912-a0da-487a-a001-401ffe37df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_document('files/Learn_Java.pdf')                # note that it is also able to load online PDFs. just pass a URL to the PDF to PyPDFLoader().\n",
    "                                                                        \n",
    "print(data[20].page_content)         # The data is splitted by pages and you can use indexes to display a specific page. This is second page because it starts from zero.\n",
    "print(data[20].metadata)             # metadata is a dictionary.\n",
    "print(f'You have {len(data)} pages in your data')         # Number of pages\n",
    "print(f' There are {len(data[20].page_content)} characters in the page')                      #Number of characters in one page\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f5f431-9345-4624-8d78-2251bd081b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_document('files/java_notes.docx')     # here data is a list with a single element and content is the page_content attribute\n",
    "\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77954882-a3c8-44b7-82c5-3bc85dfaeac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chandrayaan-3 ( CHUN-drə-YAHN) is the third mission in the Chandrayaan programme, a series of lunar-exploration missions developed by the Indian Space Research Organisation (ISRO). The mission consists of a Vikram lunar lander and a Pragyan lunar rover  was launched from Satish Dhawan Space Centre on 14 July 2023. The spacecraft entered lunar orbit on 5 August, and India became the first country to touch down near the lunar south pole, at 69°S, the southernmost lunar landing  on 23 August 2023 at 18:03 IST (12:33 UTC), made ISRO the fourth space agency to successfully land on the Moon, after Roscosmos, NASA, and the CNSA. \n",
      "Chandrayaan-3 was launched from Satish Dhawan Space Centre on 14 July 2023. The spacecraft entered lunar orbit on 5 August, and became the first lander to touch down near the lunar south pole on 23 August at 18:03 IST (12:33 UTC), making India the fourth country to successfully land on the Moon, and at 69°S, the southernmost lunar landing, until IM-1 landed further southwards in Malapert A crater on 22 February 2024. The lander was not built to withstand the cold temperatures of the lunar night, and sunset over the landing site ended the surface mission twelve days after landing. The propulsion module, still operational, transited back to a high Earth orbit from lunar orbit on 22 November 2023 for continued scientific observations of Earth.\n",
      "\n",
      "\n",
      "== History ==\n",
      "On 22 July 2019, ISRO launched Chandrayaan-2 on board a Launch Vehicle Mark-3 (LVM3) launch vehicle consisting of an orbiter, a lander and a rover. The lander was scheduled to touch down on the lunar surface on 6 September 2019 to deploy the Pragyan rover. The lander lost contact with mission control, deviated from its intended trajectory while attempting to land near the lunar south pole, and crashed.\n",
      "The lunar south pole region holds particular interest for scientific exploration. Studies show large amounts of ice there. The ice could contain solid-state compounds that would normally melt under warmer conditions elsewhere on the Moon—compounds which could provide insight into lunar, Earth, and Solar System history. Mountains and craters create unpredictable lighting that protect the ice from melting, but they also make landing there a challenging undertaking for scientific probes. For future crewed missions and outposts, the ice could also be a source of oxygen, of drinking water as well as of fuel due to its hydrogen content.\n",
      "The European Space Tracking network (ESTRACK), operated by the European Space Agency (ESA), and Deep Space Network operated by Jet Propulsion Laboratory (JPL) of NASA are supporting the mission.\n",
      "Under a new cross-support arrangement, ESA tracking support could be provided for upcoming ISRO missions such as those of India's first human spaceflight programme, Gaganyaan, and the Aditya-L1 solar research mission. In return, future ESA missions will receive similar support from ISRO's own tracking stations.\n",
      "For the first time on the lunar surface, a laser beam from NASA's Lunar Reconnaissance Orbiter was broadcast on 12 December 2023, and it was reflected back by a tiny NASA retroreflector on board the Vikram lander. The purpose of the experiment was to determine the retroreflector's surface location from the moon's orbit. The Chandrayaan-3 lander's Laser Retroreflector Array (LRA) instrument began acting as a location marker close to the lunar south pole. Through multinational cooperation, the LRA was housed on the Vikram lander. On a hemispherical support framework, it consists of eight corner-cube retroreflectors. This array enables any orbiting spacecraft equipped with appropriate instruments to use lasers ranging from different directions. The 20 gram passive optical instrument is intended to survive for several decades on the lunar surface.\n",
      "\n",
      "\n",
      "== Objectives ==\n",
      "ISRO's mission objectives for the Chandrayaan-3 mission are:\n",
      "\n",
      "Engineering and implementing a lander to land safely and softly on the surface of the Moon.\n",
      "Observing and demons\n"
     ]
    }
   ],
   "source": [
    "#data = load_from_wikipedia('Chandrayaan-3')\n",
    "data = load_from_wikipedia('Chandrayaan-3', 'en')  #Important Note: The training data for GPT-4 was cut off in September 2021. Chandrayaan-3 was launched in July 2023. So it was not included in the GPT-4 training data. Without loading the data from external sources, LLMs like gpt-3.5-turbo or gpt-4 have no knowledge of it.\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "825620a6-9437-4439-9a93-a00fa96bbcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "Pragyan (from Sanskrit: prajñāna, lit. 'wisdom') is a lunar rover that forms part of Chandrayaan-3, a lunar mission developed by the Indian Space Research Organisation (ISRO).\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))\n",
    "print(chunks[20].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd603071-0bbe-4c04-a711-dacea35baa69",
   "metadata": {},
   "source": [
    "We are using Openai's Model \"text-embedding-ada-002\" which has a cost. So in the following cell, we are calculating the embedding costs using tiktoken library, in advance to avoid any surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce21e1a-d89c-4d6d-8023-509eb5eacbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3abddb23-7906-45ef-8794-654c92898435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all indexes ....\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51789ae2-53c0-4b03-aad7-31b493a19b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index chandrayaan3 and embeddings ...Ok\n",
      "<langchain_community.vectorstores.pinecone.Pinecone object at 0x00000186B186BDD0>\n"
     ]
    }
   ],
   "source": [
    "index_name = 'chandrayaan3'\n",
    "vector_store = insert_or_fetch_embeddings(index_name, chunks)\n",
    "print(vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fd4e52-f415-4e86-ae49-b040be3eb7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = \"when the propulsion module returned to a high Earth orbit from lunar orbit?\"\n",
    "q = \"from where chandrayaan 3 was launched?\"\n",
    "print(vector_store)\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3e287f8-2c98-4f3f-8b04-1e7396589a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Quit or Exit to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #1:  where was the chandrayaan launched?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Chandrayaan-3 was launched from the Satish Dhawan Space Centre Second Launch Pad in Sriharikota, Andhra Pradesh, India.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #2:  when the propulsion module returned to a high Earth orbit from lunar orbit?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The propulsion module returned to a high Earth orbit from lunar orbit on 22 November 2023.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #3:  where is Jaipur?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Jaipur is a city in the state of Rajasthan, India.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #4:  what is dal bati?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Dal Bati is a traditional Rajasthani dish from India. It consists of a lentil curry (dal) served with baked round bread rolls (bati). The batis are typically cooked in a charcoal or wood-fired oven, giving them a unique taste and texture. It is a delicious and hearty dish popular in the Rajasthan region of India.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question #5:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting.....bye bye!\n"
     ]
    }
   ],
   "source": [
    "#creating a loop so that the user can send questions to the application continuously\n",
    "import time\n",
    "i = 1\n",
    "print('Write Quit or Exit to quit.')\n",
    "while True:\n",
    "    q = input(f'Question #{i}: ')\n",
    "    i = i + 1\n",
    "    if q.lower() in ['quit', 'exit']:\n",
    "        print('Quitting.....bye bye!')\n",
    "        time.sleep(2)\n",
    "        break\n",
    "\n",
    "    answer = ask_and_get_answer(vector_store, q)\n",
    "    print(f\"\\nAnswer: {answer['result']}\")\n",
    "    print(f'\\n{\"-\" * 50}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6689039-fa22-4904-afba-d63fbbc2de48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
